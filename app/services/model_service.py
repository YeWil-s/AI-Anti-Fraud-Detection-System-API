"""
AIæ¨¡å‹æœåŠ¡å±‚
"""
import os
import onnxruntime as ort
import joblib
import numpy as np
import librosa  # [æ–°å¢]
import io       # [æ–°å¢]
import random
from typing import Dict, Optional
from pathlib import Path
from transformers import BertTokenizer
from app.core.config import settings
from app.core.logger import get_logger

# åˆå§‹åŒ–æ¨¡å—çº§ logger
logger = get_logger(__name__)

class MockOnnxSession:
    """Mock Session (ä¿æŒä¸å˜)"""
    def __init__(self, model_type="unknown"):
        self.model_type = model_type
    def get_inputs(self):
        class Node: pass
        n = Node(); n.name = "input"; return [n]
    def run(self, output_names, input_feed):
        # æ¨¡æ‹Ÿè¿”å›
        return [np.array([[0.1, 0.9]], dtype=np.float32)]

class ModelService:
    """AIæ¨¡å‹åŠ è½½ä¸æ¨ç†æœåŠ¡"""
    
    def __init__(self):
        # --- éŸ³é¢‘æ¨¡å‹ç»„ä»¶ ---
        self.voice_session = None
        self.gnb_model = None
        self.nmf_model = None
        self.svm_model = None
        
        # --- å…¶ä»–æ¨¡å‹ ---
        self.video_session = None
        self.text_session = None
        self.tokenizer = None
        
        self._load_models()
    
    def _load_models(self):
        """åŠ è½½æ¨¡å‹æ–‡ä»¶"""
        try:
            # 1. åŠ è½½æ·±åº¦æµ (ONNX)
            if Path(settings.VOICE_MODEL_PATH).exists():
                self.voice_session = ort.InferenceSession(
                    settings.VOICE_MODEL_PATH,
                    providers=['CPUExecutionProvider']
                )
                logger.info(f"âœ“ Voice Deep Model loaded: {settings.VOICE_MODEL_PATH}")
            else:
                logger.warning(f"âš  Voice model not found at {settings.VOICE_MODEL_PATH}, using Mock.")
                self.voice_session = MockOnnxSession("voice")

            # 2. åŠ è½½ç»Ÿè®¡æµ (PKL)
            ml_path = Path("./models/ml") # ç¡®ä¿è·¯å¾„å¯¹åº”
            try:
                if (ml_path / "gnb.pkl").exists():
                    self.gnb_model = joblib.load(ml_path / "gnb.pkl")
                    self.nmf_model = joblib.load(ml_path / "nmf.pkl")
                    self.svm_model = joblib.load(ml_path / "svm.pkl")
                    logger.info("âœ“ Voice Statistical Models loaded")
                else:
                    logger.warning("âš  ML models not found in models/ml/")
            except Exception as e:
                logger.error(f"Failed to load ML models: {e}")

            # === 3. åŠ è½½è§†é¢‘æ¨¡å‹ (ONNX) ===
            if Path(settings.VIDEO_MODEL_PATH).exists():
                self.video_session = ort.InferenceSession(
                    settings.VIDEO_MODEL_PATH, 
                    providers=['CPUExecutionProvider']
                )
                logger.info(f"âœ… Video Model loaded: {settings.VIDEO_MODEL_PATH}")
            else:
                logger.warning(f"âš ï¸ Video model not found, using Mock.")
                self.video_session = MockOnnxSession("video")
                
            # === 4. åŠ è½½æ–‡æœ¬æ¨¡å‹ (BERT ONNX + Tokenizer) ===
            # [æ–°å¢è°ƒè¯•ä»£ç ] æ‰“å°ç»å¯¹è·¯å¾„ï¼Œçœ‹çœ‹ Celery åˆ°åº•åœ¨è¯»å“ªé‡Œ
            abs_path = os.path.abspath(settings.TEXT_MODEL_PATH)
            logger.info(f"[DEBUG] Loading Text Model from: {abs_path}")
            
            if Path(settings.TEXT_MODEL_PATH).exists() and Path(settings.TEXT_VOCAB_PATH).exists():
                try:
                    # [æ–°å¢è°ƒè¯•ä»£ç ] æ‰“å°æ–‡ä»¶å¤§å°
                    size = os.path.getsize(settings.TEXT_MODEL_PATH) / (1024 * 1024)
                    logger.info(f"[DEBUG] File Size: {size:.2f} MB")

                    self.tokenizer = BertTokenizer.from_pretrained(settings.TEXT_VOCAB_PATH)
                    
                    self.text_session = ort.InferenceSession(
                        settings.TEXT_MODEL_PATH, 
                        providers=['CPUExecutionProvider']
                    )
                    
                    # [æ–°å¢è°ƒè¯•ä»£ç ] è®© Celery äº²å£å‘Šè¯‰æˆ‘ä»¬å®ƒçœ‹åˆ°çš„å…¥å£æ˜¯ä»€ä¹ˆ
                    input_names = [i.name for i in self.text_session.get_inputs()]
                    logger.info(f"[DEBUG] Actual Input Names in Memory: {input_names}")
                    
                    logger.info(f"âœ… Text Fraud Model loaded")
                except Exception as e:
                    logger.error(f"âŒ Error loading Text model details: {e}")
                    self.text_session = MockOnnxSession("text")

        except Exception as e:
            logger.error(f"Error loading models: {e}")

    def _calculate_risk_level(self, probability: float, threshold: float) -> str:
        if probability >= 0.9: return "critical"
        elif probability >= threshold: return "high"
        elif probability >= (threshold / 2): return "medium"
        else: return "low"

    async def predict_voice(self, audio_bytes: bytes) -> Dict:
        """
        å£°éŸ³ä¼ªé€ æ£€æµ‹ (æ¥æ”¶åŸå§‹éŸ³é¢‘ bytes -> å†…éƒ¨æå–åŒæµç‰¹å¾ -> èåˆ)
        """
        score_dl = 0.5
        score_ml = 0.5
        has_dl = False
        has_ml = False

        try:
            # === 0. ç»Ÿä¸€é¢„å¤„ç† ===
            # ä½¿ç”¨ librosa åŠ è½½ (é‡é‡‡æ ·åˆ° 16k)
            y, sr = librosa.load(io.BytesIO(audio_bytes), sr=16000)
            if len(y) < 100:
                return {"confidence": 0.0, "is_fake": False, "error": "Audio too short"}

            # === Stream A: æ·±åº¦æµ (ResNet) ===
            # éœ€æ±‚: Log-Mel Spectrogram (1, 1, 64, Time)
            if self.voice_session:
                # 1. æå– Mel è°±å›¾ (å‚æ•°éœ€ä¸è®­ç»ƒæ—¶ dataset_loader.py ä¸€è‡´)
                mel_spec = librosa.feature.melspectrogram(
                    y=y, sr=16000, n_mels=64, n_fft=1024, hop_length=512
                )
                # 2. è½¬ Log åˆ»åº¦ (AmplitudeToDB)
                log_mel = librosa.power_to_db(mel_spec, ref=np.max)
                
                # 3. æ„é€ è¾“å…¥ Tensor (Batch=1, Channel=1, Freq=64, Time=...)
                # æ³¨æ„: ONNX è¾“å…¥æ˜¯ float32
                dl_input = log_mel[np.newaxis, np.newaxis, :, :].astype(np.float32)
                
                # 4. æ¨ç†
                input_name = self.voice_session.get_inputs()[0].name
                output = self.voice_session.run(None, {input_name: dl_input})
                
                # 5. Softmax
                logits = output[0]
                probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
                score_dl = float(probs[0][1]) # Index 1 æ˜¯ Fake
                has_dl = True

            # === Stream B: ç»Ÿè®¡æµ (GNB -> NMF -> SVM) ===
            # éœ€æ±‚: MFCC (Time, 20) -> Mean/Std ç»Ÿè®¡é‡
            if self.gnb_model and self.nmf_model and self.svm_model:
                # 1. æå– MFCC
                mfcc = librosa.feature.mfcc(y=y, sr=16000, n_mfcc=20)
                
                # 2. è®¡ç®—ç»Ÿè®¡ç‰¹å¾ (Mean + Std) -> 40ç»´å‘é‡
                # æ³¨æ„: è¿™é‡Œå¿…é¡»ä¸ train_ml_stream.py é‡Œçš„ extract_features é€»è¾‘å®Œå…¨ä¸€è‡´
                mfcc_mean = np.mean(mfcc, axis=1)
                mfcc_std = np.std(mfcc, axis=1)
                ml_input = np.concatenate([mfcc_mean, mfcc_std]).reshape(1, -1)
                
                # 3. æµæ°´çº¿æ¨ç†
                prob_feats = self.gnb_model.predict_proba(ml_input) # GNB
                nmf_feats = self.nmf_model.transform(prob_feats)    # NMF
                score_ml = float(self.svm_model.predict_proba(nmf_feats)[0][1]) # SVM
                has_ml = True

            # === Fusion: å†³ç­–èåˆ ===
            if has_dl and has_ml:
                # æ·±åº¦æµæƒé‡ ï¼Œç»Ÿè®¡æµ 
                final_score = 0.5 * score_dl + 0.5 * score_ml
                method = "dual_stream_fusion"
            elif has_dl:
                final_score = score_dl
                method = "deep_learning_only"
            elif has_ml:
                final_score = score_ml
                method = "statistical_only"
            else:
                final_score = 0.0
                method = "mock"

            # ç»“æœåˆ¤å®š
            threshold = settings.VOICE_DETECTION_THRESHOLD
            return {
                "confidence": final_score,
                "is_fake": final_score > threshold,
                "risk_level": self._calculate_risk_level(final_score, threshold),
                "method": method,
                "details": {"dl_score": score_dl, "ml_score": score_ml}
            }

        except Exception as e:
            logger.error(f"Prediction failed: {e}", exc_info=True)
            return {"confidence": 0.0, "is_fake": False, "error": str(e)}

    async def predict_video(self, video_tensor: np.ndarray) -> Dict:
        """
        è§†é¢‘Deepfakeæ£€æµ‹ (ResNet+LSTM æ—¶åºæ£€æµ‹)
        """
        if self.video_session is None:
            return {"confidence": 0.0, "is_deepfake": False, "message": "Video model not loaded"}
        
        try:
            # 1. å‡†å¤‡è¾“å…¥
            input_name = self.video_session.get_inputs()[0].name
            
            # å¼ºåˆ¶è½¬ float32
            video_tensor = video_tensor.astype(np.float32)
            
            # 2. æ‰§è¡Œæ¨ç†
            outputs = self.video_session.run(None, {input_name: video_tensor})
            logits = outputs[0]
            
            # ================= [Debug æ—¥å¿—] =================
            # çœ‹çœ‹åŸå§‹è¾“å‡ºåˆ°åº•æ˜¯å¤šå°‘ã€‚å¦‚æœæ˜¯ [-120, 50]ï¼Œé‚£ Softmax è‚¯å®šå°±æ˜¯ 0.0
            logger.info(f"ğŸ§  [Model Output] Raw Logits: {logits}")
            # ====================================================

            # 3. Softmax
            exp_logits = np.exp(logits - np.max(logits))
            probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
            
            fake_prob = float(probs[0][0]) 
            
            logger.info(f"ğŸ“Š [Result] Fake Prob: {fake_prob:.6f}") # çœ‹çœ‹ç²¾ç¡®å€¼

            # ... (åç»­ä»£ç ä¸å˜)
            threshold = settings.VIDEO_DETECTION_THRESHOLD
            is_deepfake = fake_prob > threshold
            risk_level = self._calculate_risk_level(fake_prob, threshold)
            
            return {
                "confidence": fake_prob,
                "is_deepfake": is_deepfake,
                "risk_level": risk_level,
                "threshold": threshold,
                "model_version": "v2.0-lstm",
                "raw_logits": logits.tolist() # æŠŠåŸå§‹åˆ†ä¹Ÿè¿”å›å›å»
            }
            
        except Exception as e:
            logger.error(f"Video prediction failed: {e}", exc_info=True)
            return {"confidence": 0.0, "is_deepfake": False, "error": str(e)}

    def predict_text(self, text: str) -> dict:
        """æ–‡æœ¬è¯ˆéª—æ£€æµ‹"""
        # 1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
        if not self.text_session or not self.tokenizer or isinstance(self.text_session, MockOnnxSession):
            return {"risk_level": "unknown", "confidence": 0.0, "error": "Text Model not loaded"}

        try:
            # 2. é¢„å¤„ç† (Tokenization)
            inputs = self.tokenizer(
                text,
                return_tensors="np",
                padding="max_length",
                truncation=True,
                max_length=256
            )
            
            ort_inputs = {
                "input_ids": inputs["input_ids"].astype(np.int64),
                "attention_mask": inputs["attention_mask"].astype(np.int64)
            }

            # 3. æ¨ç†
            logits = self.text_session.run(None, ort_inputs)[0]
            
            # 4. Softmax
            def softmax(x):
                e_x = np.exp(x - np.max(x))
                return e_x / e_x.sum()

            probs = softmax(logits[0])
            fraud_prob = float(probs[1]) # Label 1 æ˜¯è¯ˆéª—

            # 5. ç»“æœåˆ¤å®š (ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é˜ˆå€¼)
            threshold = settings.TEXT_DETECTION_THRESHOLD
            is_fraud = fraud_prob > threshold
            
            return {
                "risk_level": self._calculate_risk_level(fraud_prob, threshold),
                "label": "fraud" if is_fraud else "normal",
                "confidence": round(fraud_prob, 4),
                "threshold": threshold,
                "keywords": ["é«˜é£é™©è¯­ä¹‰"] if is_fraud else []
            }

        except Exception as e:
            logger.error(f"Text prediction error: {e}")
            return {"risk_level": "error", "details": str(e)}

model_service = ModelService()